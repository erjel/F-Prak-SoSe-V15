{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuch V-So 15 – Maschinelles Lernen in der wissenschaftlichen Bildanalyse\n",
    "\n",
    "# Einleitung\n",
    "\n",
    "Ziel dieses Versuchs ist es physikalische Eigenschaften\n",
    "- *Position*\n",
    "- *lokale Dichte*\n",
    "- *Volumen*\n",
    "- *Intensität*\n",
    "- usw.\n",
    "\n",
    "von Zellen mit Deep Learning aus Mikroskopaufnahmen zu extrahieren. Der Versuch ist im wesentlich in vier Phasen geteilt:\n",
    "1. Einführung\n",
    "2. Vorbereitung der Trainingsdaten\n",
    "3. Erstellen von geeigneten Netzwerken\n",
    "4. Visualisierung der Daten\n",
    "\n",
    "# Hinweise\n",
    "\n",
    "Dies ist keine Prüfungssituation!\n",
    "- Zu jedem Zeitpunkt kann eine [Suchmaschine](https://en.wikipedia.org/wiki/Web_search_engine) euer Vorankommen beschleunigen.\n",
    "- Wenn ein Problem auftritt, dann sucht nach Beispiele und Lösungen auf [Github](https://github.com/), [StackOverflow](https://stackoverflow.com/) und in offiziellen Dokumentationen (z.B. [Matplotlib](https://matplotlib.org/gallery/index.html)).\n",
    "- Es erwartet keiner, dass ihr alle Python [Befehle](https://docs.python.org/3/library/) auswendig kennt. Es wird aber erwartet, dass ihr in kurzer Zeit euch neue Befehle heraussuchen könnt.\n",
    "- Wenn ihr Fragen habt, lasst es uns wissen.\n",
    "- Speichert das Notebook, immer wenn ihr es für notwendig haltet.\n",
    "\n",
    "# 1. Einführung\n",
    "\n",
    "## Macht euch mit der Maschine vertraut:\n",
    "\n",
    "Die für die Numerik sind folgende Komponenten von Bedeutung:\n",
    "- CPU (Model)\n",
    "- Arbeitsspeicher (Speicherplatz)\n",
    "- Speicher (Model (von nvme0n1))\n",
    "- GPU (Model)\n",
    "\n",
    "Ohne diese Hardwarekomponenten könnte der Versuch nicht durchgeführt werden. Sie sollten deshalb im Protokoll Erwähnungen finden. Mit den folgenden Befehlen könnt ihr sie ganz einfach auslesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo | \\\n",
    "awk -v FS=':' '                                       \\\n",
    "  /^physical id/ { if(nb_cpu<$2)  { nb_cpu=$2 } }     \\\n",
    "  /^cpu cores/   { if(nb_cores<$2){ nb_cores=$2 } }   \\\n",
    "  /^processor/   { if(nb_units<$2){ nb_units=$2 } }   \\\n",
    "  /^model name/  { model=$2 }                         \\\n",
    "                                                      \\\n",
    "  END{                                                \\\n",
    "   nb_cpu=(nb_cpu+1);                                 \\\n",
    "   nb_units=(nb_units+1);                             \\\n",
    "                                                      \\\n",
    "   print \"CPU model:\",model;                          \\\n",
    "   print nb_cpu,\"CPU,\",nb_cores,\"physical cores per CPU, total\",nb_units,\"logical CPU units\" \\\n",
    " }' # Quelle: https://superuser.com/questions/388115/interpreting-output-of-cat-proc-cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat /sys/class/block/nvme0n1/device/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=gpu_name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macht euch mit der Daten vertraut\n",
    "\n",
    "Im Gegensatz zu vielen anderen Versuche im Rahmen des Fortgeschrittenenpraktikums, ist ein Großteil der für diesen Versuch notwendigen Daten bereits vorhanden. Ihr findet die Daten unter:\n",
    "\n",
    "\n",
    "**Traininginput** - Mikroskopaufnahmen.\n",
    "\n",
    "`/DATA/IN/<Experiment Name>/`\n",
    "\n",
    "**Traininglabels (Ground Truth)** - Von den Mikroskopaufnahmen erstellte Binärmasken.\n",
    "\n",
    "`/DATA/GT/<Experiment Name>/`\n",
    "\n",
    "**Anwendungsdatensätze** - Datensätze für eure Analyse.\n",
    "\n",
    "`/DATA/validation/noise1000/`\n",
    "\n",
    "**Notebook & Benutzerdaten** - Speicherort für *.ipynb* und allen Daten die ihr im Laufe des Versuchs generiert.\n",
    "\n",
    "`/tf/Notebooks`\n",
    "\n",
    "#### Aufgabe 1:\n",
    "- Benutzt das [Kommandozeilenprogramm](https://linux.die.net/man/1/ls) `ls` um euch mit dem Inhalt der Verzeichnisse vertraut zu machen.\n",
    "- <font id=\"overview\">Nutzt</font> die unten definierten Funktionen `loadImage` und `zSlicer` um  2 verschiedenen Trainingslabels und -Inputpaare (also ingesamt 4 Diagramme) anzuzeigen.\n",
    "\n",
    "Hinweise:\n",
    "\n",
    "- Mit vorangestellten `!` könnt ihr in diesem Notebook beliebige Programme auf dem Computer ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import imread\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loadImage(filepath):\n",
    "    \"\"\"\n",
    "    Liest tiff-Dateien von der Festplatte als numpy array in den Arbeitsspeicher.\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    filepath : string\n",
    "        Vollständiger Pfad der tiff-Datei.\n",
    "        \n",
    "    \n",
    "    Rückgabewert\n",
    "    ------------\n",
    "    out : array_like\n",
    "        Tiff-Datei als 3D numpy Array.\n",
    "    \"\"\"    \n",
    "    \n",
    "    return np.asarray(imread.imload_multi(filepath))\n",
    "\n",
    "def zSlicer(*args):\n",
    "    \"\"\"\n",
    "    Zeichnet ein interactives Diagramm einer beliebigen Anzahl an 3D oder 2D numpy Arrays.\n",
    "    \n",
    "    Über einen Slider unter dem Diagramm kann die dargestellte z-Ebene verändert werden.\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    *args : array_like\n",
    "        Ein (oder mehrere komma-getrennte) 2D bzw. 3D numpy Array(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_img = len(args)\n",
    "            \n",
    "    num_rows = 1\n",
    "    num_cols = len(args)\n",
    "    \n",
    "    # init plot\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=[9.5,5])\n",
    "    \n",
    "    val = 0\n",
    "    aspect = 'equal'\n",
    "    \n",
    "    plots = []\n",
    "    for i, img in enumerate(args):\n",
    "        if len(args) == 1:\n",
    "            ax = axes\n",
    "        else:\n",
    "            ax = axes[i]\n",
    "        \n",
    "        if img.ndim > 2:\n",
    "\n",
    "            plots.append([ax.imshow(img[val], aspect=aspect, vmin=np.amin(img[1:]), vmax=np.amax(img[1:])), img])\n",
    "\n",
    "        else:\n",
    "            ax.imshow(img, aspect=aspect, vmin=np.amin(img), vmax=np.amax(img))\n",
    "            \n",
    "\n",
    "    def update_plot(change):\n",
    "        val = change['new']\n",
    "        for plot,img in plots:\n",
    "            plot.set_array(img[val])\n",
    "            \n",
    "        return\n",
    "\n",
    "\n",
    "    slider = widgets.IntSlider(value=val, max=args[-1].shape[0]-1)\n",
    "    display(slider)\n",
    "\n",
    "    slider.observe(update_plot, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitiere CUDA auf eine GPU\n",
    "\n",
    "Im Verlauf dieses Versuchs werden Grafikkarten benutzt um Tensoroperationen durchzuführen. Dies soll mithilfe des [*tensorflow*](https://www.tensorflow.org/) Python Moduls passieren. Standardmäßig nutzt *tensorflow*  fast den gesamten verfügbare Grafikspeicher.\n",
    "\n",
    "Die momentane Auslastung der Grafikkarten auf den System lassen sich mit dem Programm *nvidia-smi* überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über die [Umgebungsvariable](https://de.wikipedia.org/wiki/Umgebungsvariable) *CUDA_VISIBLE_DEVICES* kann man die Geräte einschränken, die für [CUDA](https://de.wikipedia.org/wiki/CUDA) Berechnungen benutzt werden dürfen.\n",
    "\n",
    "`%env` ist *jupyter* [*Magie*](https://ipython.readthedocs.io/en/stable/interactive/magics.html) um Umgebungsvariablen zu setzen oder anzeigen zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit GPU usage to actually needed memory #https://kobkrit.com/using-allow-growth-memory-option-in-tensorflow-and-keras-dc8c8081bc96\n",
    "from tensorflow.keras.backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Einführung\n",
    "\n",
    "[Keras](https://keras.io/) ist eine high-level Deep Learning Bibliothek, die es erlaubt eigene Designs für neuronale Netzwerke zu basteln, ohne das *TensorFlow* oder gar *CUDA* Code geschrieben werden muss. Dabei kann Keras mit einer Vielzahl von Deep Learning Backends benutzt werden. Wir werden es ausschließlich mit dem Tensorflow Backend benutzen.\n",
    "\n",
    "Im Laufe dieses Versuchs wollen wir Schritt für Schritt einen [Autoencoder](http://science.sciencemag.org/content/313/5786/504/tab-pdf) erstellen, der dem Design von [UNet](http://arxiv.org/abs/1505.04597) entspricht.\n",
    "\n",
    "### Die Keras Model API\n",
    "\n",
    "Keras bezeichnet jedes Netzwerk unabhängig von der Architektur als [*Modell*](https://keras.io/models/model/).\n",
    "Die Modell API von Keras kommen einr [Reihe](https://keras.io/models/model/) von nützlichen Methoden, die uns [Kompilieren](https://keras.io/getting-started/sequential-model-guide/#compilation), [Training](https://keras.io/getting-started/sequential-model-guide/#training), und [Vorhersagen](https://keras.io/models/model/#predict) mit dem Netzwerk erleichtern.\n",
    "\n",
    "#### Beispiel: Erstellen eines Modells\n",
    "\n",
    "Ein Modell wird erstellt indem man die von Keras in `tensorflow.keras.layers` zur Verfügung gestellten Schichten ineinander schachtelt. Dabei wird jeder Schicht (mit Ausnahme der Inputschicht) die vorherige Schicht als Argument übergeben.\n",
    "\n",
    "Im folgenden Beispiel definieren wir uns zunächst eine (Python-) Funktion, die für uns ein sehr einfaches Convolutionary Neural Network (CNN) erstellt. Dabei wird drei Mal hintereinander eine 2D [Konvolution](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) auf einen Input der Größe $64 \\times 64 \\times 1$ (z.B. ein Graustufenbild mit $64 \\times 64$ Pixel) angewendet.\n",
    "\n",
    "$$f^{(1)}: \\mathbb{R}^{64\\times 64 \\times 1} \\longmapsto \\mathbb{R}^{64\\times 64\\times 1}\\\\ \\pmb{x} \\longrightarrow \\pmb{x} \\ast \\pmb{k}_1\\\\\\\\ f^{(2)}: \\mathbb{R}^{64\\times 64 \\times 1} \\longmapsto \\mathbb{R}^{64\\times 64\\times 1}\\\\ \\pmb{x} \\longrightarrow \\pmb{x} \\ast \\pmb{k}_2\\\\\\\\ f^{(3)}: \\mathbb{R}^{64\\times 64 \\times 1} \\longmapsto \\mathbb{R}^{64\\times 64\\times 1}\\\\ \\pmb{x} \\longrightarrow \\pmb{x} \\ast \\pmb{k}_3 \\tag{2}\\label{eq:simple_model_functions}$$\n",
    "Das Netzwerk $N$ ist die Verknüpfung dieser Funktionen:\n",
    "$$N(\\pmb{x}) = (f^{(3)} \\circ f^{(2)} \\circ f^{(1)})(\\pmb{x}) \\tag{3} \\label{eq:simple_model}\\\\ =  f^{(3)}(f^{(2)}(f^{(1)}(\\pmb{x})))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def simple_model(input_shape=(64, 64, 1)):\n",
    "\n",
    "    # Eingabeschicht des Models; definiert mit welchen Größe von Elementen das Netzwerk arbeiten kann.\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Erste Schicht; bekommt die Eingabeschicht als Argument und die Funktionsdefinition als Paramameter übergeben.\n",
    "    f1 = Conv2D(1, 2, padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # Zweite (verdeckte) Schicht; vgl. erste Schicht.\n",
    "    f2 = Conv2D(1, 2, padding='same', activation='relu')(f1)\n",
    "    \n",
    "    # Dritte (Ausgabe-)Schicht; vfl. erste Schicht.\n",
    "    f3 = Conv2D(1, 2, padding='same', activation='relu')(f2)\n",
    "\n",
    "    # Für die Initalisierung des Models wird die Eingabe- und Ausgabeschicht explizit angegeben.\n",
    "    return Model(inputs=inputs, outputs=f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei jedem Aufruf der Funktion `simple_model` bekommen wir als Rückgabewert ein fertig initalisiertes `Model`-Objekt geliefert. Die Methoden `summary` von dem Objekt `simple_model` können wir z.B. nutzen um uns eine Übersicht über die Netzwerkarchitektur anzeigen zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nutzt die soeben definierte Funktion um ein neues Modell zu initialisieren.\n",
    "simple_model = simple_model()\n",
    "\n",
    "# Die Funktion `summary()` ist Teil der Model-API und gibt uns eine Übersicht für das Modell aus.\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Kompilieren eines Modells\n",
    "\n",
    "Diese Modell existiert zur Zeit nur als Python Objekt im Arbeitsspeicher. Um die volle Leistungsfähigkeit der Hardware auszunutzen, müssen wir das Python Objekt in Maschinensprache übersetzen (sprich: kompilieren). Dieser Prozess wird uns komplett von der Methode `compile` abgenommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "simple_model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode benötigt bereits bei der Kompilierung die Angabe zu der Optimierungsfunktion (hier: [Adam](https://arxiv.org/abs/1412.6980v8) mit einer Anpassungsrate von 0.0001) und zu der Verlustfunktion (hier: [Binäre Kreuz-Entropie](https://de.wikipedia.org/wiki/Kreuzentropie)). Zusätzlich kann man eine Metrik angeben, welche die Leistungsfähigkeit des Modells während des Trainings und Testens misst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Trainieren eines Modells\n",
    "\n",
    "Zum Training des Modells benötigen wir Trainingsdaten. In dem folgenden Versuch wollen wir Mikroskopbilder und die zugehörigen Binärmasken von Zellen als Trainingsdaten benutzen. In diesem Beispiel reduzieren wir uns auf Zufallsdaten. Dabei werden insgesamt 1000 Datenpaare für das Training erzeugt (vgl. erste Dimension im folgende Beispiel). `X` stellt die Sammlung an Eingabedaten und `Y` die Sammlung der Labeldaten dar. Die weiteren Dimensionen sind durch die Breite des Netzwerks (bzw. Bilddimensionen der Inputdaten) vorgebeben. Wenn man Bilder als Eingabedaten benutzt stellt die letzte Dimension die Anzahl der Kanäle dar. Bei einem [Graustufenbild](https://www.google.com/search?q=Graustufenbild&source=lnms&tbm=isch) ist dies 1 bei einem [RGB](https://de.wikipedia.org/wiki/RGB-Farbraum) Bild ist die Anzahl 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = np.random.random((1000, 64, 64, 1)), np.random.random((1000, 64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = simple_model.fit(x=X, y=Y, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Visualisieren eines Trainingsprozesses\n",
    "\n",
    "Die Methode `fit` erzeugt einen Rückgabewert, den wir im vorherigen <a href=\"#Beispiel:-Trainieren-eines-Modells\">Beispiel</a> in der Variable `history` gespeichert haben. `history` enthält für jede Epoche den Trainingsverlauf der Trainingsparameter (hier: *loss* und *acc*). Diese sind für die spätere Evaluation der Konvergenz sehr nützlich und können zu anschauliche Diagramme aufbereitet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "modelname = 'Beispielnetzwerk'\n",
    "\n",
    "parameter = history.history.keys() \n",
    "YLabels = history.history.keys() \n",
    "\n",
    "# Plotting\n",
    "f, axes = plt.subplots(1, len(parameter), figsize=(4.74*len(parameter), 5))\n",
    "\n",
    "for ax, param, ylabel in zip(axes, parameter, YLabels):\n",
    "    y = history.history[param]\n",
    "    x = np.arange(1, len(y)+1)\n",
    "    ax.plot(x, y, 'ro', label=modelname)\n",
    "    ax.set_xlabel('Epoche')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    ax.set_title(ylabel)\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Netzwerk zur Vorhersage benutzen\n",
    "\n",
    "Auch wenn die Konvergenz von Netzwerken in Abhängigkeit von verschiedensten Parameter ein sehr interessantes eigenes [Forschungsfeld](https://arxiv.org/list/cs.AI/recent) darstellt, sind wir eher an der Anwendung der durch das Netzwerk gelernten Funktion interessiert.\n",
    "\n",
    "Unser Beispielmodell hat sich im Laufe des Trainingsprozesses nicht verändert. Wir haben ein Netzwerk ohne freie Parameter erstellt. Das Modell entspricht weiterhin der Identität im $\\mathbb{R}^{m\\times64\\times64\\times1}$.\n",
    "\n",
    "In dem folgenden Beispiel wollen wir dennoch dieses Netzwerk benutzen um die \"gelernte\" Identität auf neu erzeugte Inputdaten anzuwenden. Die Inputdaten sind in diesen Fall durch [Schachbrettmuster](https://stackoverflow.com/a/51715491) mit einem zufälligen Skalierungsfaktor gegeben.\n",
    "\n",
    "Mit der Methode `predict` wenden wir das Netzwerk auf die Daten an. Beachtet hierbei, dass die Anzahl der Inputdaten (hier: 5) nicht notwendigerweise die Anzahl der Trainingspaare (hier: 1000) sein muss. Wir können  das Netzwerk auf einer beliebigen Anzahl von Vorhersagen gleichtzeitig benutzen. Dabei muss nur beachtet werden, dass das komplette Modell inkl. Trainingsdaten in den [Grafikkartenspeicher](#Macht-euch-mit-der-Maschine-vertraut:) passt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Erstelle ein Schachberttmuster\n",
    "chess_board = np.indices((64, 64)).sum(axis=0) % 2\n",
    "\n",
    "# Erzeuge eine zusätzliche Dimension für den Farbkanal\n",
    "chess_board_reshaped = chess_board.reshape(64, 64, 1)\n",
    "\n",
    "# Erzeuge einen neues Array für die Eingabedaten\n",
    "X2 = np.zeros((5, 64, 64, 1))\n",
    "    \n",
    "# Fülle das Eingabearray mit Schachbrettern und einen zufälligen Faktor\n",
    "for i in range(5):\n",
    "    X2[i] = chess_board_reshaped*np.random.random(1)\n",
    "\n",
    "# Nutze unser Beispielmodell um die gelernte Funktion auf die Eingabedaten anzuwenden\n",
    "results = simple_model.predict(X2)\n",
    "\n",
    "# Nutze die bereits definierte Funktion um die Resultate mit den Eingaben zu Vergleichen\n",
    "zSlicer(X2[..., 0], results[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Netzwerk und Trainingsverlauf für die spätere Auswertung/ Anwendung speichern\n",
    "\n",
    "Wir werden später mehrere Netzwerke mit verschiedenen Anzahlen an Schichten zu trainieren. Unser Ziel wird sein das beste dieser Netzwerke für die Anwendung zu benutzen. Dazu müssen wir die Netzwerke und deren Trainingsverlauf auf der Festplatte speichern.\n",
    "\n",
    "Über die unten definierten Funktionen `save_trained_model` und `load_trained_model` könnt ihr sehr einfach eure Netzwerke auf der Festplatte archivieren.\n",
    "\n",
    "`save_trained_model` kann neben dem Netzwerkdesign (als [json](https://de.wikipedia.org/wiki/JavaScript_Object_Notation)) und den gelernten Gewichten (als [hdf5](https://de.wikipedia.org/wiki/Hierarchical_Data_Format)) auch den in `history` gespeicherten Trainingsverlauf (als [csv](https://de.wikipedia.org/wiki/CSV_(Dateiformat))) abspeichern.\n",
    "\n",
    "`load_trained_model` erstellt aus dem gespeicherten Netzwerkdesign und den gespeicherten Gewichten ein neues, fertig trainiertes Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def save_trained_model(history, net, foldername):\n",
    "    \n",
    "    folder = os.path.join(foldername, datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    \n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "    hist_pivot = [dict(zip(history.history, col)) for col in zip(*history.history.values())] # https://stackoverflow.com/a/37489316\n",
    "        \n",
    "    history_path = os.path.join(folder, 'history.csv')\n",
    "    \n",
    "    print('Save history in path \"{}\"'.format(history_path))\n",
    "    with open(os.path.join(folder, 'history.csv'), 'w') as f:\n",
    "        w = csv.DictWriter(f, history.history.keys())\n",
    "        w.writeheader()\n",
    "        for row in hist_pivot:\n",
    "            w.writerow(row)\n",
    "    \n",
    "    model_json = net.to_json()\n",
    "    model_path = os.path.join(folder, \"model.json\")\n",
    "    print('Save model in path \"{}\"'.format(model_path))\n",
    "    with open(model_path, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    weights_path = os.path.join(folder, \"weights.h5\")\n",
    "    print('Save weights in path \"{}\"'.format(weights_path))\n",
    "    net.save_weights(weights_path)\n",
    "    \n",
    "    return folder\n",
    "    \n",
    "def load_trained_model(foldername):\n",
    "    \n",
    "    # load json and create model\n",
    "    print('Load trained model from \"{}\"'.format(foldername))\n",
    "    with open(os.path.join(foldername, 'model.json'), 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    net = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    net.load_weights(os.path.join(foldername, 'weights.h5'))\n",
    "    \n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model, weights, and history\n",
    "save_folder = save_trained_model(history, simple_model, 'beispielnetzwerk')\n",
    "\n",
    "# ... do something else ...\n",
    "\n",
    "# Load model and weights\n",
    "new_simple_model = load_trained_model(save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun stellt sich die Frage: Wie trainieren wir eigentlich ein Netzwerk auf unseren Datensätzen?\n",
    "\n",
    "# Trainingsdaten einlesen\n",
    "\n",
    "Wie bereits <a href=\"#Macht-euch-mit-der-Daten-vertraut\">erwähnt</a>, liegen die Labeldaten unter `/DATA/GT/` und zugehörige Inputdaten unter `/DATA/IN/`. Um Trainingsdatenpaare zu erstellen muss man für jedes einzelne Paar die entsprechenden Dateipfade angeben. Damit die von uns erstellten Netzwerke auch mit unbekannte Daten umgehen können, benötigen wir mehrere Hundert Paare. Nur so enthalten unsere Trainingsdaten eine akzeptable Entropie.  \n",
    "\n",
    "Weil ein manuelles Abschreiben dieser Dateipfade den Zeitrahmen diese Versuchs sprengen würde, lassen wir uns alle Dateipfade ausgeben, die einem bestimmten Muster entsprechen. Dazu benutzen wir die `glob`-Funktion des gleichnamigen [Moduls](https://docs.python.org/3/library/glob.html).\n",
    "\n",
    "#### Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls /DATA/GT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filelist_labels = glob.glob('/DATA/GT/*/*.tif')\n",
    "filelist_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Zeichen *\\** bedeutet hier eine beliebige Anzahl beliebiger Zeichen. D.h, wir erhalten mit einem Befehl alle tif-Dateien in der zweiten Unterverzeichnissebene unter `DATA/GT/`.\n",
    "\n",
    "#### Aufgabe 2\n",
    "- Lasst euch alle tif-Dateipfade der Inputdaten anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfache Pfadoperationen\n",
    "\n",
    "Mit der Ausgabe von Dateilisten haben wir noch kein Trainingspaar erstellt. Dazu müssen wir für jeden Eintrag in der Liste der Labeldatenpfade den zugehörigen Pfad in der Liste der Eingabedatenpfade finden. Dabei müssen gleiche Indizes zu zusammengehörigen Datensätze gehören.\n",
    "\n",
    "Operationen zur Modifikation von Verzeichnissnamen befinden sich im Modul [`os.path`](https://docs.python.org/3.7/library/os.path.html) der Python Standard Library. Wir benötigen in diesem Versuch:\n",
    "- `basename` - extrahiert einen Dateiname (bzw. Namen des letzten Unterverzeichnisses, wenn ein Verzeichniss angegeben ist).\n",
    "- `dirname` - gibt den Namen des Elternverzeichnis des gegebenen Pfades zurück.\n",
    "- `isfile` - überprüft ob eine Datei existiert und gibt *True* oder *False* zurück.\n",
    "- `join` - fügt mehrere Zeichenketten zu einen Pfad zusammen.\n",
    "\n",
    "#### Aufgabe 3\n",
    "\n",
    "1. Erstellt euch ein einfaches Skript um zwei Dateilisten zu erstellen.\n",
    "    - `x_filelist` beinhaltet alle Inputdatenpfade.\n",
    "    - `y_filelist` beinhaltet alle zugehörigen Labeldatenpfade.\n",
    "2. Lasst euch die Anzahl aller vollständigen Input-/Labelpaare ausgeben.  \n",
    "\n",
    "\n",
    "Hinweis:\n",
    "- Achtet darauf, dass *Experiment Name* und *Dateiname* in den beiden Pfadlisten für identische Indizes ebenfalls identisch sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilderausschnitte extrahieren\n",
    "\n",
    "Wir wollen Netzwerke erstellen, die (wie das  Beispielnetzwerk `simple_model`), mit Inputdateien der Größe $64 \\times 64 \\times 1$ Pixel arbeiten. Die auf der Festplatte gespeicherten tif-Dateien sind aber übereinander angeordnete 2D Mikroskopaufnahmen. Folglich sind die eingelesenen Arrays 3-dimensional. Der vom Netzwerk verarbeitet Input muss 4D sein.\n",
    "\n",
    "Dabei definiert die erste Dimension das Bild welches wir betratchten, die zweite Dimension ist die $y$-Postion im Bild, die dritte Dimension die $x$-Position im Bild und die letzte Dimension den Pixelwert für einen bestimmten Kanal.\n",
    "\n",
    "Die höhere Anzahl an Pixel in den auf der Festplatte gespeicherten Volumen ist kein Nachteil. Im Gegenteil: Sie eröffnen die Möglichkeit, dass wir durch die Auswahl von zufälligen $x$, $y$ und $z$-Ausschnitt) innerhalb des Volumens die Entropie unserer Trainingsdaten weiter erhöhen können und damit gelernte Modell bei der Anwendung robuster sein werden. (In anderen Worten: Der Raum der Inputdaten auf den das Modell gute Vorhersagen ermitteln kann wird größer.)\n",
    "\n",
    "Bleibt die Frage wie wir Bilder aus den Volumen ausschneiden können. Dazu kann [Indexing](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.indexing.html#other-indexing-options) von Numpy Arrays benutzt werden.\n",
    "\n",
    "Aufgrund der benutzten Aktivierungsfunktion ist es vorteilhaft die Inputdaten auf einen bestimmten Wertebereich zu normieren. In der Praxis hat sich *zero mean unit variance* (deutsch: Mittelwert von 0 und Varianz von 1) als Wertebereich bewährt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 4\n",
    "1. Erstellt zwei große Arrays `X` und `Y`\n",
    "    - `X` beinhaltet Inputbilder mit der Form $64 \\times 64 \\times 1$.\n",
    "    - `Y` beinhaltet die den Inputbildern zugeordneten Labelbilder.\n",
    "   Dazu:\n",
    "        - [Iteriert](https://docs.python.org/3/tutorial/controlflow.html#for-statements) über alle verfügbare Datenpaare in den Dateilisten.\n",
    "        - Schneidet an [zufälligen](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.randint.html) Positionen in $x$, $y$ und $z$ Bilder mit der $x$, $y$ Form von $64 \\times 64$ Pixel aus.\n",
    "        - Normiert jedes ausgeschnitte **Inputbild** auf einen [Mittelwert](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html#numpy.mean) von 0 und einer [Standardabweichung](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.std.html) von 1 (= Varianz von 1).\n",
    "        - [Formt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) die Bilder in numpy Arrays der Form $64 \\times 64 \\times 1$ um und speichert sie in `X` in `Y`.\n",
    "    \n",
    "2. Nutzt die unten definierte Funktion `plotBatches` um euch die ersten 10 Bilder von `X` und `Y` anzeigen zu lassen.\n",
    "\n",
    "Hinweis: \n",
    "- Die Dimensionen sind umgekehrt gereiht: $(z, y, x)$.\n",
    "- Nutzt den numpy [Befehl]((https://docs.scipy.org/doc/numpy/reference/constants.html#numpy.newaxis)) `newaxis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plotBatches(noisy, gt):\n",
    "    \"\"\"Entpackt eine Sammlung von Bildern in mehrere imshow Diagramme.\"\"\"\n",
    "    \n",
    "    f, axes = plt.subplots(noisy.shape[0], 2, figsize=(9.5, noisy.shape[0]*4.5))\n",
    "    for i in range(noisy.shape[0]):\n",
    "        (ax1, ax2) = axes[i, :]\n",
    "        ax1.imshow(np.squeeze(noisy[i, :]))\n",
    "        ax2.imshow(np.squeeze(gt[i, :]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt für Schritt zum Autoencoder\n",
    "\n",
    "Im Folgenden wollen wir ein Netzwerk mit ungefähr 98% Genauigkeit erstellen.\n",
    "\n",
    "Dazu testen wir 5 verschiedene Architekturen. Jede Architektur unterscheidet sich durch die Anzahl der `Pooling` und `UpSampling2D` Schichten. Unten ist eines dieser Netzwerke (`autoencoder1`) mit jeweils einer Pooling und UpSampling Schicht bereits vorgegeben. Beachtet hierbei dass die Funktionen `pooling` und `upsampling` mehrere Netzwerkschichten beinhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, UpSampling2D\n",
    "\n",
    "def pooling(inputlayer, num_filter):\n",
    "    pool = MaxPooling2D(pool_size=(2,2))(inputlayer)\n",
    "    conv = Conv2D(num_filter, 3, activation='relu', padding='same')(pool)\n",
    "    output = Conv2D(num_filter, 3, activation='relu', padding='same')(conv)\n",
    "        \n",
    "    return output\n",
    "\n",
    "def upsampling(inputlayer, mergelayer, num_filter):\n",
    "    up = UpSampling2D(size=(2,2))(inputlayer)\n",
    "    conv1 = Conv2D(num_filter, 2, activation='relu', padding='same')(up)\n",
    "    merge = Concatenate(axis=3)([mergelayer, conv1])\n",
    "    conv2 = Conv2D(num_filter, 3, activation='relu', padding='same')(merge)\n",
    "    output = Conv2D(num_filter, 3, activation='relu', padding='same')(conv2)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def autoencoder1(input_shape=(64, 64, 1)):\n",
    "    \n",
    "    inputs = Input(input_shape)\n",
    "    num_filter = 64\n",
    "    \n",
    "    conv1 = Conv2D(num_filter, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(num_filter, 3, activation='relu', padding='same')(conv1)\n",
    "    \n",
    "    pool1 = pooling(conv1, num_filter*2)\n",
    "    \n",
    "    up1 = upsampling(pool1, conv1, num_filter)\n",
    "\n",
    "    outputs = Conv2D(2, 3, activation='relu', padding='same')(up1)\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(outputs)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemäß der obigen Definition ist `autoencoder1` wie folgt aufgebaut:\n",
    "1. `Input`\n",
    "2. 2x `Conv2D` (64 Kanäle)\n",
    "3. `pooling` (128 Kanäle) \n",
    "4. `upsampling` (64 Kanäle, `Concatenate` mit Ergebnis von Schicht in 2.) )\n",
    "5. 2x `Conv2D` (Ausgabeschichten)\n",
    "    \n",
    "Wir wollen weitere Netzwerke mit folgenden Architekturen aufbauen:\n",
    "\n",
    "##### autoencoder0\n",
    "(entspricht `autoencoder1` ohne `pooling` und `upsampling`):\n",
    "1. `Input`\n",
    "2. 2x `Conv2D` (64 Kanäle)\n",
    "4. 2x `Conv2D` (64 Kanäle)\n",
    "8. 2x `Conv2D` (Ausgabeschichten)\n",
    "    \n",
    "##### autoencoder2\n",
    "(entspricht `autoencoder1` mit jeweils ein zusätzlichen `pooling` und `upsampling` Schritt)\n",
    "1. `Input`\n",
    "2. 2x `Conv2D`\n",
    "3. `pooling` (128 Kanäle) \n",
    "4. `pooling` (256 Kanäle) \n",
    "5. `upsampling`(128 Kanäle, `Concatenate` mit Ergebnis von Schicht in 3.)\n",
    "6. `upsampling`(64 Kanäle, `Concatenate` mit Ergebnis von Schicht in 2.)\n",
    "7. 2x `Conv2D` (Ausgabeschichten)\n",
    "\n",
    "##### autoencoder3\n",
    "(entspricht `autoencoder1` mit jeweils zwei zusätzlichen `pooling` und `upsampling` Schritten)\n",
    "1. `Input`\n",
    "2. 2x `Conv2D`\n",
    "3. `pooling` (128 Kanäle) \n",
    "4. `pooling` (256 Kanäle) \n",
    "5. `pooling` (512 Kanäle)\n",
    "6. `upsampling` (256 Kanäle, `Concatenate` mit Ergebnis von Schicht in 4.)\n",
    "7. `upsampling` (128 Kanäle, `Concatenate` mit Ergebnis von Schicht in 3.)\n",
    "8. `upsampling` (64 Kanäle, `Concatenate` mit Ergebnis von Schicht in 2.)\n",
    "9. 2x `Conv2D` (Ausgabeschichten)\n",
    "\n",
    "\n",
    "##### autoencoder4\n",
    "(entspricht `autoencoder1` mit jeweils drei zusätzlichen `pooling` und `upsampling` Schritten)\n",
    "1. `Input`\n",
    "2. 2x `Conv2D`\n",
    "3. `pooling` (128 Kanäle) \n",
    "4. `pooling` (256 Kanäle) \n",
    "5. `pooling` (512 Kanäle)\n",
    "6. `pooling` (1024 Kanäle)\n",
    "8. `upsampling` (512 Kanäle, `Concatenate` mit Ergebnis von Schicht in 5.)\n",
    "9. `upsampling` ( 256 Kanäle, `Concatenate` mit Ergebnis von Schicht in 4.)\n",
    "10. `upsampling` (128 Kanäle, `Concatenate` mit Ergebnis von Schicht in 3.)\n",
    "11. `upsampling` (64 Kanäle, `Concatenate` mit Ergebnis von Schicht in 2.)\n",
    "12. 2x `Conv2D` (Ausgabeschichten)\n",
    "\n",
    "Dabei ist zu beachten, dass die Ausgabeschichten, die Inputschicht und die ersten beiden Konvolutionen unverändert bleiben.\n",
    "\n",
    "#### Aufgabe 5\n",
    "- Erstellt Funktionen für die Netzwerke `autoencoder0`, `autoencoder2`, `autoencoder3` und `autoencoder4`.\n",
    "- <a href=\"#Beispiel:-Kompilieren-eines-Modells\">Kompiliert</a> die Modelle mit\n",
    "    - der Adam Optimierungsfunktion mit einer Anpassungsrate von `1e-4`\n",
    "    - der Verlustfunktion `binary_crossentropy'\n",
    "    - der Metrik `accuracy`\n",
    "- <a href=\"#Beispiel:-Trainieren-eines-Modells\">Trainiert</a> die Modelle mit\n",
    "    - `X`, `Y` als Input- bzw. Labeldaten\n",
    "    - einem Validationsanteil von 10%\n",
    "    - für 100 Epochen\n",
    "- <a href=\"#Beispiel:-Netzwerk-und-Trainingsverlauf-für-die-spätere-Auswertung/-Anwendung-speichern\">Speichert</a> die trainierten Modelle auf der Festplatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <a href=\"#Aufgabe-5\">Aufgabe 5</a> wurden eine ganze Reihe von Netzwerken erstellt und trainiert. Für den weiteren Verlauf des Praktikums wollen wir nun das beste Netzwerk auswählen. Die Frage ist nun: Welches kann am besten mit unbekannte Mikroskopdaten umgehen. Die Daten der <a href=\"#Aufgabe-5\">gespeicherten</a> Trainingsläufe könnt ihr mit der unten definierten Funktion `readHistory` von der Festplatte lesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def readHistory(foldername, latest=True):\n",
    "    history_files = glob.glob(os.path.join(foldername, '*', 'history.csv'))\n",
    "    \n",
    "    if latest:\n",
    "        dates = np.asarray([os.stat(history_file).st_ctime for history_file in history_files])\n",
    "        idx = np.argsort(dates)[-1]\n",
    "        data = np.genfromtxt(history_files[idx], delimiter=',', skip_header=1)\n",
    "        header = np.genfromtxt(history_files[idx], delimiter=',', max_rows=1, dtype=str)\n",
    "        \n",
    "        return data, header\n",
    "    \n",
    "    else:\n",
    "        data = [np.genfromtxt(filename, delimiter=',', skip_header=1) for filename in history_files]\n",
    "        header = [np.genfromtxt(filename, delimiter=',', max_rows=1, dtype=str) for filename in history_files]\n",
    "        \n",
    "        return data, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 6\n",
    "- <a href=\"#Beispiel:-Visualisieren-eines-Trainingsprozesses\">Erstellt</a> zwei Diagramme:\n",
    "    1. Verlauf der Trainingsgenauigkeit über die Trainingsepochen.\n",
    "    2. Verlauf der Validierungsgenauigkeit über die Trainingsepochen.\n",
    "- Ladet mit der Funktion `load_trained_model` das Modell mit der besten Validierungsgenauigkeit in das aktuelle Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treffe Voraussagen auf noch unbekannte Daten\n",
    "\n",
    "Im zweiten Teil des Praktikums wollen wir uns auf eine wesentliche Fragestellung in unserer Arbeitsgruppe konzentrieren:\n",
    "\n",
    "- Welche physikalischen Zelleigenschaften können wir aus noch unbekannten Mikroskopaufnahmen extrahieren?\n",
    "\n",
    "Dazu sollt ihr **eine** Mikroskopaufnahmen aus `/DATA/validation/noise1000/` auswählen und im weiteren Verlauf verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls /DATA/validation/noise1000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotropische Auflösung\n",
    "\n",
    "Um wirklich quantitative Aussagen über Zellen zu treffen (Volumen, Intensität, Abstände), müssen wir beachten, dass\n",
    "durch das Mikroskopsetup in unterschiedliche Raumrichtungen unterschiedliche Auflösungen existieren. In den Beispielbildern wird von einem Pixelabstand von $63,2\\,\\mathrm{nm}$ in $x$ und $y$-Richtung und von $400\\,\\mathrm{nm}$ in $z$-Richtung ausgegangen.\n",
    "\n",
    "Für isotropische Auflösungen wollen müssen wir den Pixelabstand in $z$ Richtung auf ebenfalls $63,2\\,\\mathrm{nm}$ linear interpolieren.\n",
    "\n",
    "Dazu bietet sich die [Funktion](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html) `affine_transform` im Modul `scipy.ndimage` an. Die wesentlichen benötigten Parameter sind:\n",
    " - Das Usprungsvolumen.\n",
    " - Die Transformationsmatrix (eine $3 \\times 3$ [Diagonalmatrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html#numpy.diag), die auf der Hauptdiagonale die Einträg $\\left(d, 1, 1\\right)$ besitzt, wobei $d$ der inverse Streckungsfaktor in $z$-Richtung darstellt).\n",
    " - `output_shape` muss angegeben werden. Dieser ist bis auf die Anzahl an $z$-Ebenen identisch mit der ursprünglichen Array-Form.\n",
    " - `order=1` welches die Interpolation auf die lineare Ordnung beschränkt.\n",
    "\n",
    "#### Aufgabe 7\n",
    "- Berechnet die lineare Interpolation von einer ausgewählten .tif Dateien in `/DATA/validation/`\n",
    "- [Speichert](https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html) die Interpolation als `.npy`-Datei auf der Festplatte.\n",
    "- Erstellt eine [Maximumsprojektion](https://docs.scipy.org/doc/numpy/reference/generated/numpy.amax.html#numpy.amax) aller 3 Raumachsen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das trainierte Netzwerk geht von an zufälligen Positionen im Volumen extrahierten 2D Minibatches mit der Größe von $64 \\times 64$ Pixel aus.\n",
    "\n",
    "Dies kann nicht für die Vorhersage des kompletten Volumens benutzt werden. Wir müssen das Volumen in kleine Teilbilder zerschneiden. Dabei ist es wichtig, dass eine festgelegte Reihenfolge eingehalten wird, um hinterher die Vorhersagen wieder zu einem kompletten Volumen zusammensetzen zu können.\n",
    "\n",
    "#### Aufgabe 8\n",
    "- Nutzt das trainierte Modell und `predictImageStack` um in den interpolierten Volumen die Zellen zu erkennen.\n",
    "- Speichert als `.npy`-Datei`:\n",
    "    - die Zellvorhersagen\n",
    "    - die Intensitätsbilder\n",
    "- Berechnet von jeder Zellvorhersage eine [Durchschnittsprojektion](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) in $x$, $y$ und $z$-Richtung.\n",
    "- Erstellt ein Histogramm der Pixelwerte in der Vorhersage\n",
    "\n",
    "Hinweis: Benutzt für das Histogram [logarithmische Auftragung](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_yscale.html) auf der y-Achse und mindestens 100 Abschnitte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictImageStack(img, model, num_channels=1, num_classes=1, batch_size=32):\n",
    "    print('Image shape: {}'.format(img.shape))\n",
    "    input_shape = model.layers[0].output_shape\n",
    "    print('Model input shape: {}'.format(input_shape))\n",
    "    \n",
    "    in_z, in_y, in_x = img.shape\n",
    "    height, width = input_shape[1:3]\n",
    "    nz = in_z - 1\n",
    "    ny = int(np.floor(in_y / height))\n",
    "    nx = int(np.floor(in_x / width))\n",
    "    \n",
    "    print('Number of tiles in x, y, z: ({:d}, {:d}, {:d})'.format(nx, ny, nz))\n",
    "    \n",
    "    margin_x = (in_x - (width * nx)) / 2\n",
    "    margin_y = (in_y - (height * ny)) / 2\n",
    "    \n",
    "    \n",
    "    img = img[..., None]\n",
    "    imgs = ()\n",
    "    for _ in range(num_channels):\n",
    "        imgs = imgs + (img,)\n",
    "    img = np.concatenate(imgs, axis=-1)\n",
    "    \n",
    "    if margin_x == 0.0:\n",
    "        x_start = None\n",
    "        x_end = None\n",
    "        \n",
    "    else:\n",
    "        x_start = int(np.floor(margin_x))\n",
    "        x_end = -int(np.ceil(margin_x))\n",
    "    \n",
    "    if margin_y == 0.0:\n",
    "        y_start = None\n",
    "        y_end = None\n",
    "    else:\n",
    "        y_start = int(np.floor(margin_y))\n",
    "        y_end = -int(np.ceil(margin_y))\n",
    "        \n",
    "    # cut away overview plane in z and reduce size in x, y to fit model input\n",
    "    img_ = img[:, y_start:y_end, x_start:x_end, :]\n",
    "    \n",
    "    print('cut image to shape: {}'.format(img_.shape))\n",
    "    \n",
    "    num_batches = int(nx * ny * nz)\n",
    "    \n",
    "    print('Total number of tiles: {}'.format(num_batches))\n",
    "    \n",
    "    prediction_batches = np.zeros((num_batches, height, width, num_channels))\n",
    "    \n",
    "    print('Prediction batches shape: {}'.format(prediction_batches.shape))\n",
    "    \n",
    "    def norm(a, axis=None):\n",
    "        a = a - np.mean(a, axis=axis, keepdims=True)\n",
    "        return a / np.std(a, axis=axis, keepdims=True)\n",
    "    \n",
    "    \n",
    "    print('Fill input batches with image data')\n",
    "    k = 0\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            for l in range(nz):\n",
    "                prediction_batches[k, :, :, :] = norm(img_[l, height*i:height*(i+1), width*j:width*(j+1)], axis=(0, 1, 2))\n",
    "                k += 1\n",
    "                \n",
    "    num_predictions = num_batches // batch_size\n",
    "    \n",
    "    print('Predict batches')\n",
    "    for i in range(num_predictions):\n",
    "        prediction_batches[i*batch_size:(i+1)*batch_size] = model.predict(prediction_batches[i*batch_size:(i+1)*batch_size])\n",
    "    \n",
    "    if num_predictions*batch_size != num_batches:\n",
    "        prediction_batches[num_predictions*batch_size:] = model.predict(prediction_batches[num_predictions*batch_size:])\n",
    "    \n",
    "    img_prediction = np.zeros((*img_.shape[:3], num_classes))\n",
    "    \n",
    "    print('Constuct predicted image with shape: {}'.format(img_prediction.shape))\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            for l in range(nz):\n",
    "                img_prediction[l, height*i:height*(i+1), width*j:width*(j+1)] = prediction_batches[k, ..., :num_classes]\n",
    "                k += 1\n",
    "\n",
    "    \n",
    "    return np.squeeze(img_prediction), np.squeeze(img_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung\n",
    "\n",
    "**Die Auswertung ist mit den gespeicherten Werten auch Zuhause oder im Lernzentrum möglich.**\n",
    "\n",
    "\n",
    "Mit den Vorhersagen wollen wir jetzt weiterarbeiten. Momentan stehen wir noch vor einigen Problemen:\n",
    "1. Die Vorhersage muss diskretisiert werden (von \\[0, 1\\] zu \\{0, 1\\})\n",
    "2. Wir wollen Objekte, die nicht miteinander verbunden sind mit unterschiedlichen [Label](http://scikit-image.org/docs/dev/api/skimage.measure.html#label) versehen.\n",
    "\n",
    "#### Aufgabe 9:\n",
    "- Erstellt eine Liste, die für jede abgespeicherte Vorhersage die zugehörige Labelmatrix enthält. Dazu:\n",
    "    - Vergleicht jedes Pixel mit einem von euch definierten Grenzwert\n",
    "    - Erstellt die Labelmatrix auf Grundlage des Vergleichs.\n",
    "\n",
    "Hinweis: \n",
    "- Die [Vergleichsoperatoren](https://www.tutorialspoint.com/python/python_basic_operators.htm) können nicht nur Skalarwerte miteinander vergleichen, sondern funktionieren auch beim Vergleich von numpy Arrays mit Skalarwerten. Das Resultat ist dann eine Matrix aus *True* und *False* entsprechend der Vergleichsoperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus diesen segmentierten Daten kann man schon viele Eigenschaften berechnen. Im Detail sind dies:\n",
    "- Zellpositionen (Beispiel)\n",
    "- Mittlere/ Maximale/ Minimale  Helligkeit \n",
    "- Zellvolumen\n",
    "- Lokale Dichte (mit bereitgestellten Funktion)\n",
    "- Zellabstände (optional)\n",
    "- Nemantic Orderparamter (optional)\n",
    "\n",
    "## Zelleigenschaften bestimmen\n",
    "\n",
    "#### Beispiel\n",
    "\n",
    "Sei `w` eine oben bestimmte Labelmatrix (bzw. das Labelvolumen) und `img_` die der Segmentierung zugehörige Intensitätswerte, dann lässt sich ein numpy Array `centroids_array` mit allen Schwerpunkten aller Objekten in der Labelmatrix bestimmen via:\n",
    "```python\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "props = regionprops(w, img_)\n",
    "centroids_list = [props[i].centroid for i in range(len(props))]\n",
    "centroids_array = np.asarray(centroids_list)\n",
    "```\n",
    "\n",
    "Dabei beinhaltet das Ergebnis von `skimage.measure.regionprops` noch eine ganze Reihe von weiteren [Objekteigenschaften](http://scikit-image.org/docs/dev/api/skimage.measure.html#regionprops)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 10\n",
    "- Bestimmt folgende für jedes gelabelte Volumen\n",
    "    - alle Zellpositionen\n",
    "    - alle mittlere, maximale und minimale Helligkeit der gelabelten Zellen\n",
    "    - alle Zellvolumen\n",
    "    \n",
    "- Erstellt für jedes der Labelvolumen und jeden (skalarwertigen) Parameter ein entsprechendes Histogram.\n",
    "\n",
    "Hinweis:\n",
    "- Der für das Volumen benötigte Objekteingenschaft heißt *area*, da usprünglich nur 2D Eigenschaften extrahiert werden konnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lokale Dichte\n",
    "\n",
    "Obwohl Position, Helligkeit und Volumen interessante Zellparameter sind, können wir aus den Segmentierung noch wesentlich mehr Informationen gewinnen. In den meisten Veröffentlichungen konzentiert man sich auf einige wenige Eigenschaften. Wirklich relevante Wissenschaft produziert man wenn nur, wenn man die ausgetretenen Pfade verlässt. Dazu muss man sich überlegen wie man Messwerte aus den Bildern ermitteln kann, für die noch keine vorgefertigte Funktion existiert.\n",
    "\n",
    "Ein solches Beispiel wäre die lokale Dichte. Eine Beispielimplementierung findet ihr in der Funktion `calculateLocalDensity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def calculateLocalDensity(w, centroid_array, radius = 60):\n",
    "    \"\"\" Calculates occupied volume in a sphere around centroid\n",
    "    \"\"\"\n",
    "\n",
    "    # Erstellt ein Koordinatengitter\n",
    "    Z, Y, X = np.meshgrid(np.arange(-radius, radius, 1), np.arange(-radius, radius, 1), np.arange(-radius, radius, 1))\n",
    "    \n",
    "    # Wandelt das 3D Gitter in eine Sammlung der Gitterpunktkoordinaten um\n",
    "    Z, Y, X = np.ravel(Z), np.ravel(Y), np.ravel(X)\n",
    "    \n",
    "    # Bestimmt für jede Gitterpunktkoordinate ob sie Teil der Kugel ist oder nicht.\n",
    "    sphere = norm([Z, Y, X], axis=0) < radius\n",
    "    \n",
    "    # Das (einheitenlose) Volumen der Kugel ist die Summe alle Kugelvoxel\n",
    "    vol_sphere = np.sum(sphere) \n",
    "\n",
    "    # Für die lokale Dichte wollen wir jeden Zellvoxel (unabhängig von dem Labelwert) mit 1 bezeichnen\n",
    "    w_ = w > 0\n",
    "    \n",
    "    # Damit wir nicht durch Randeffekte das Ergebnis verfälschen,\n",
    "    # ergänzen wir an den Volumenrändern Voxel mit dem Wert -1\n",
    "    w_ = np.pad(w_, radius, 'constant', constant_values=-1)\n",
    "\n",
    "    # Erzeugen den Ausgabevektor\n",
    "    localDensity = np.zeros(centroid_array.shape[0])\n",
    "    \n",
    "    # Iterieren über Zellpositionen\n",
    "    for i in range(centroid_array.shape[0]):\n",
    "        z, y, x = centroid_array[i]\n",
    "        z, y, x = np.round([z, y, x])\n",
    "        \n",
    "        # Schneiden um jede Zellvolumen einen Würfel mit der Kantenlänge 2*radius aus\n",
    "        w_part = w_[int(z):int(z+2*radius), int(y):int(y+2*radius), int(x):int(x+2*radius)]\n",
    "        \n",
    "        # Nur Beiträge die Teil der Kugel sind sollen zur Berechnung der lokalen Dichte beitragen \n",
    "        vol = w_part.flatten()*sphere\n",
    "        \n",
    "        # Beiträge die mit -1 versehen sind sind außerhalb der Labelmatrix und sollen nicht zum Volumen\n",
    "        out = np.sum(vol == -1)\n",
    "        \n",
    "        # Berechnung der lokalen Dichte\n",
    "        localDensity[i] = np.sum(vol==1)/(vol_sphere - out);\n",
    "\n",
    "    return localDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierung\n",
    "\n",
    "\n",
    "Durch die Mikroskopie kennen wir nicht nur den Wert von Zelleigenschaften, sondern auch die räumliche Verteilung. Um diese zu Visualisieren eignen sich u.A. 3D Scatterplots.\n",
    "\n",
    "In der *matplotlib* [Dokumentation](https://matplotlib.org/examples/mplot3d/scatter3d_demo.html) findet ihr ein Beispiel dazu. Wir haben uns erlaubt diese Beispiel weiter zu vereinfachen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "n = 100\n",
    "\n",
    "x = np.random.randn(n)\n",
    "y = np.random.randn(n)\n",
    "z = np.random.randn(n)\n",
    "\n",
    "fig = plt.figure(figsize=(9.5, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "ax.set_title('Beispiel 3D Scatter Plot')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 11\n",
    "- Erstellt für die 3 Labelvolumen ein 3D Scatterplot für die durchschnittliche Helligkeit und die lokale Dichte.\n",
    "    - Die Markerposition soll durch den jeweiligen Schwerpunkt gegeben sein.\n",
    "    - Die Markerfarbe soll den Parameter darstellen.\n",
    "    \n",
    "Hinweis:\n",
    "- Berechnet die lokale Dichte mit der gegebenen Funktion `calculateLocalDensity`.\n",
    "- Ihr könnt über die Farbe der Punkte (`c=`) die Intensität oder die Dichte kodieren.\n",
    "- die `x, y, z` erhaltet ihr über die oben ermittelten Schwerpunkte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Rendering (optional)\n",
    "\n",
    "Zum Abschluss stehen euch zwei Möglichkeiten offen 3D Visualisierungen der Zellen zu erstellen.\n",
    "\n",
    "## *matplotlib* (nicht empfohlen)\n",
    "\n",
    "Kann in diesem Notebook gemacht werden, dauert aber lange und die Plots lassen sich nicht wirklich interativ drehen.\n",
    "\n",
    "**Bitte entfernt die Anführungszeichen nur, wenn ihr bereit seit sehr lange auf das Ergebnis zu warten!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import marching_cubes_lewiner\n",
    "\n",
    "verts, faces, normals, values = marching_cubes_lewiner(dilate, 0.5)\n",
    "\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_trisurf(verts[:, 0], verts[:,1], faces, verts[:, 2],\n",
    "                cmap='Spectral', lw=1)\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *ParaView* (empfohlen)\n",
    "\n",
    "ParaView  ist ein 3D Rendering Programm für große Datensätze und de facto Standard für die Visualisierung von aufwendigen Simulation. Die folgende Handreichung soll euch dazu ermuntern ParaView zum Rendern einer 3D Darstellung zu benutzen. (beispielsweise kann man mit ParaView Abbildungen wie z.B. in der [Versuchsbeschreibung](https://www.uni-marburg.de/de/fb13/studium/praktika/praktika-fuer-physiker/v-so-15-maschinelles-lernen) erstellen oder gar ganze Simulationsreihen in einen [Film](https://static-content.springer.com/esm/art%3A10.1038%2Fs41567-018-0356-9/MediaObjects/41567_2018_356_MOESM3_ESM.mp4) umwandeln)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to VTI\n",
    "from pyevtk.hl import imageToVTK\n",
    "\n",
    "imageToVTK('prediction', pointData={\"prediction\":w})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "Ladet bitte den ParaView Installer von der offiziellen [Website](https://www.paraview.org/download/) herunter. Im Folgenden wird die Windowsversion benutzt. Für Max OSX und der Linux Distribution eurer Wahl finden sich entsprechende Versionen im Mac App Store und der Paketverwaltung.\n",
    "\n",
    "## Datentransfer\n",
    "\n",
    "Um die Daten auf euren lokalen Computer herunterladen zu können, könnt ihr die jupyter [Homepage](http://localhost:8888/)  benutzen. Wesentlich komfortabler geht es aber über ein Programm, welches Daten über eine [SSH](https://de.wikipedia.org/wiki/Secure_Shell)-Verbindung übertragen kann. Unter Mac OSX und Linux könnt ihr euren normalen Dateimanager benutzen. Unter Windows benötigt ihr ein zusätzliches Programm (z.B. [WinSCP](https://winscp.net/eng/download.php)).\n",
    "\n",
    "Für eine Verbindung mit WinSCP benötigt Ihr folgende Daten:\n",
    "- File protocol: SFTP\n",
    "- Host name: ***\n",
    "- User name: ***\n",
    "- Passwort: ***\n",
    "\n",
    "Unter nautilus (Linux Dateimanager) findet ihr das entsprechende Menü unter 'Other Locations' und 'Connect to Server'. In die Addresszeile müsst ihr eintragen:\n",
    "`sftp://<username>@<hostname>/`\n",
    "\n",
    "Bitte ladet die `.vti`-Dateien unter `/home/<username>/Notebooks` herunter\n",
    "\n",
    "## Datenvisualisierung in ParaView\n",
    "\n",
    "* Öffnet die Datei.\n",
    "* Nutzt den Threshold Filter im Bereich eurer Labels.\n",
    "* Unter Coloring `prediction` auswählen.\n",
    "* Spielt ein bisschen mit den Einstellungen im Bereich *OSPRay Rendering* bis ihr eine schöne 3D Visualizierung erstellt habt.\n",
    "\n",
    "Hinweise: \n",
    "* Für Schatten benötigt man eine Fläche die Schatten abbilden.\n",
    "* *path tracer* ist wesentlich schöner anzusehen, kostet aber auch sehr viel Rechenleistung.\n",
    "\n",
    "# Abschluss\n",
    "\n",
    "1. Speichert ALLE Daten/ Notebooks/ DIAGRAMME\n",
    "2. Ladet ALLE .ipynb, .npy, .vti herunter\n",
    "3. Ladet alle .tif Bilder herunter\n",
    "\n",
    "\n",
    "# Sonstiges\n",
    "\n",
    "- Wenn Ihr weiter mit Deep Learning/ Python Notebooks experimentieren möchtet, aber keinen Computer mit GPU zur Verfügung habt:<br></br>\n",
    "https://colab.research.google.com/notebooks/welcome.ipynb (Account wird benötigt)\n",
    "\n",
    "- Der Jupyter Notebook Server ist Teil der Anaconda Distribution:<br></br>\n",
    "https://www.anaconda.com/distribution/ (ist im Lernzentrum installiert)\n",
    "\n",
    "- Wenn ihr eine leistungsstarken (Linux) Computer zur Verfügung habt und euch keine Arbeit mit dem Erstellen einer jupyter/ tensorflow/ GPU-Umgebung machen wollt:<br></br>\n",
    "https://www.tensorflow.org/install/docker (Für das Praktikum verwendete tag: 1.13.1-gpu-py3-jupyter)\n",
    "\n",
    "- Die aktuelle Version dieses Notebooks findet ihr sowohl im Fachbereichs [Git-Repository](https://git.physik.uni-marburg.de/Jelli/f_praktikum_sose_v15) (Intranet) und auf [Github](https://github.com/erjel/F-Prak-SoSe-V15). Verbesserungsvorschläge sind immer willkommen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
